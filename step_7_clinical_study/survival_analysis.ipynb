{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.image_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Local module imports\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_artery_ann\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry_annotation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (clean_geojson_annotations, \n\u001b[1;32m     20\u001b[0m                                              get_outer_cnt)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils_measure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m measure_thickness\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.image_utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geojson\n",
    "import openslide\n",
    "import cv2\n",
    "import logging\n",
    "from typing import List, Tuple, Optional\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, module='pandas')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.image_utils import plot_artery_ann\n",
    "from utils.geometry_annotation_utils import (clean_geojson_annotations, \n",
    "                                             get_outer_cnt)\n",
    "from utils.utils_measure import measure_thickness\n",
    "\n",
    "from utils.utils_stat_process import post_process\n",
    "from utils.utils_stat_feature import extract_features\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for file paths\n",
    "VESSEL_PAT_INFO_PATH = \"/DataMount/NEPTUNE/Vessel_Project/vessel_pat_info_20240303.csv\"\n",
    "VESSEL_CLASSIFICATION_PATH = \"/workspace/vessel_ai/artery_classification/Neptune_Artery_Classification_Sheets.xlsx\"\n",
    "VESSEL_SEGMENTATION_DIR = \"/DataMount/NEPTUNE/Vessel_Project/data_selection/ann_geojson/all\"\n",
    "TRI_DIR = \"/DataMount/NEPTUNE/Vessel_Project/TRI/\"\n",
    "DIR_SAVE = \"/DataMount/NEPTUNE/Vessel_Project/cropped_arteries_by_types_w_ann\"\n",
    "THICKNESS_SAVE_PATH = os.path.join(\"/DataMount/NEPTUNE/Vessel_Project\", \"thickness_0501.json\")\n",
    "THICKNESS_CONVEX_SAVE_PATH = os.path.join(\"/DataMount/NEPTUNE/Vessel_Project\", \"thickness_convex_0501.json\")\n",
    "\n",
    "ARTERY_TYPES = [\"Arterioles\", \"Interlobular Arteries\", \"Arcuate Arteries\"]\n",
    "# Mapping dictionary for severity\n",
    "SEVERITY_MAPPING = {\n",
    "    '0 - No': 0,\n",
    "    '1 - Mild': 1,\n",
    "    '2 - Moderate': 2,\n",
    "    '3 - Severe': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = pd.read_csv(VESSEL_PAT_INFO_PATH)\n",
    "\n",
    "pat_df = pat_df[pat_df[\"WSI_Selected\"].notna() \n",
    "                & pat_df[\"ESRDorEGFR40BX_LR\"].notna() \n",
    "                & pat_df[\"DaysBXtoESRDorEGFR40_LR\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thickness_features = pd.read_csv(\"features_label_15_30_10_10_0527.csv\")\n",
    "df_thickness_features.loc[:, \"WSI_Fake_Name\"] = df_thickness_features.loc[:, \"Image Name\"].str[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artery_area_arr = df_thickness_features.loc[:, \"Artery Area\"].values\n",
    "plt.hist(np.log(artery_area_arr), bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assuming df_thickness_features is already loaded and contains the relevant data\n",
    "df_artery_type_artery_area = df_thickness_features.loc[:, [\"Image Name\", \"Artery Type\", \"Artery Area\"]]\n",
    "\n",
    "# Remove duplicates based on 'Image Name'\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area.drop_duplicates(subset='Image Name')\n",
    "\n",
    "# Ensure only the three specific artery types are included\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area_unique[\n",
    "    df_artery_type_artery_area_unique['Artery Type'].isin(['Arcuate Arteries', 'Interlobular Arteries', 'Arterioles'])\n",
    "]\n",
    "\n",
    "# Encode the artery types\n",
    "le = LabelEncoder()\n",
    "df_artery_type_artery_area_unique['Artery Type Encoded'] = le.fit_transform(df_artery_type_artery_area_unique['Artery Type'])\n",
    "\n",
    "# Prepare the data for fitting\n",
    "X = df_artery_type_artery_area_unique['Artery Area'].values.reshape(-1, 1)\n",
    "y = df_artery_type_artery_area_unique['Artery Type Encoded']\n",
    "\n",
    "# Train a Gaussian Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prepare to plot in log scale - calculate x values for PDF plotting in log scale\n",
    "\n",
    "log_x_values = np.linspace(np.log(X.min()), np.log(X.max()), 100)\n",
    "x_values = np.exp(log_x_values)  # back-transform to original scale for PDF calculation\n",
    "\n",
    "# Plotting each class's PDF and histogram on a log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = le.classes_\n",
    "\n",
    "for i in range(len(model.classes_)):\n",
    "    # Use the model's mean and variance for the original scale\n",
    "    mu, var = model.theta_[i][0], model.var_[i][0]\n",
    "    pdf = norm.pdf(x_values, mu, np.sqrt(var))\n",
    "    p_scaled = pdf * x_values  # Scale the PDF by x to adjust for the change of variable (log scale)\n",
    "\n",
    "    # Plotting histograms of the log-transformed data\n",
    "    log_group = np.log(X[y == i])\n",
    "    ax.hist(log_group, bins=len(log_group), density=True, alpha=0.6, color=colors[i])\n",
    "    # Plotting the scaled PDF\n",
    "    ax.plot(log_x_values, p_scaled, color=colors[i], label=f'{labels[i]}: μ={mu:.2f}, σ={np.sqrt(var):.2f}')\n",
    "\n",
    "# Set decision boundaries where the probabilities of two classes are equal\n",
    "# decision_boundaries = []\n",
    "# for i in range(len(model.classes_) - 1):\n",
    "#     boundary = (model.theta_[i][0] + model.theta_[i + 1][0]) / 2\n",
    "#     decision_boundaries.append(boundary)\n",
    "#     ax.axvline(np.log(boundary), color='black', linestyle='--')\n",
    "#     ax.text(np.log(boundary), ax.get_ylim()[1] * 0.5, f'Threshold\\n{boundary:.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Gaussian Fits and Histograms on Log Scale (Model Fitted on Original Scale)')\n",
    "ax.set_xlabel('Log of Artery Area')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assuming df_thickness_features is already loaded and contains the relevant data\n",
    "df_artery_type_artery_area = df_thickness_features.loc[:, [\"Image Name\", \"Artery Type\", \"Artery Area\"]]\n",
    "\n",
    "# Remove duplicates based on 'Image Name'\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area.drop_duplicates(subset='Image Name')\n",
    "\n",
    "# Ensure only the three specific artery types are included\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area_unique[\n",
    "    df_artery_type_artery_area_unique['Artery Type'].isin(['Arterioles', 'Interlobular Arteries', 'Arcuate Arteries'])\n",
    "]\n",
    "\n",
    "# Change the order of artery types for legend and boundaries\n",
    "order = ['Arterioles', 'Interlobular Arteries', 'Arcuate Arteries']\n",
    "df_artery_type_artery_area_unique['Artery Type'] = pd.Categorical(\n",
    "    df_artery_type_artery_area_unique['Artery Type'], \n",
    "    categories=order, \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Encode the artery types\n",
    "le = LabelEncoder()\n",
    "df_artery_type_artery_area_unique['Artery Type Encoded'] = le.fit_transform(df_artery_type_artery_area_unique['Artery Type'])\n",
    "\n",
    "# Prepare the data for fitting\n",
    "X = df_artery_type_artery_area_unique['Artery Area'].values.reshape(-1, 1)\n",
    "y = df_artery_type_artery_area_unique['Artery Type Encoded']\n",
    "\n",
    "# Apply log transformation to the data\n",
    "log_X = np.log(X)\n",
    "\n",
    "# Train a Gaussian Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(log_X, y)\n",
    "\n",
    "# Prepare to plot in log scale - calculate x values for PDF plotting in log scale\n",
    "log_x_values = np.linspace(log_X.min(), log_X.max(), 1000)\n",
    "x_values = np.exp(log_x_values)  # back-transform to original scale for PDF calculation\n",
    "\n",
    "# Plotting each class's PDF and histogram on a log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = le.classes_\n",
    "\n",
    "# Function to find the intersection points of two Gaussian distributions\n",
    "def find_intersection(mu1, sigma1, mu2, sigma2):\n",
    "    a = 1/(2*sigma1**2) - 1/(2*sigma2**2)\n",
    "    b = mu2/(sigma2**2) - mu1/(sigma1**2)\n",
    "    c = mu1**2/(2*sigma1**2) - mu2**2/(2*sigma2**2) + np.log(sigma2/sigma1)\n",
    "    return np.roots([a, b, c])\n",
    "\n",
    "for i in range(len(model.classes_)):\n",
    "    # Use the model's mean and variance for the original scale\n",
    "    mu, var = model.theta_[i][0], model.var_[i][0]\n",
    "    pdf = norm.pdf(log_x_values, mu, np.sqrt(var))\n",
    "    \n",
    "    # Plotting histograms of the log-transformed data\n",
    "    ax.hist(log_X[y == i], bins=len(X[y==i])//3, density=True, alpha=0.6, color=colors[i])\n",
    "    # Plotting the PDF\n",
    "    ax.plot(log_x_values, pdf, color=colors[i], label=f'{labels[i]}: μ={np.exp(mu):.2f}, σ={np.exp(np.sqrt(var)):.2f}')\n",
    "\n",
    "# Calculate and plot decision boundaries\n",
    "mu1, sigma1 = model.theta_[0][0], np.sqrt(model.var_[0][0])\n",
    "mu2, sigma2 = model.theta_[1][0], np.sqrt(model.var_[1][0])\n",
    "mu3, sigma3 = model.theta_[2][0], np.sqrt(model.var_[2][0])\n",
    "\n",
    "boundaries_1_2 = find_intersection(mu1, sigma1, mu2, sigma2)\n",
    "boundaries_2_3 = find_intersection(mu2, sigma2, mu3, sigma3)\n",
    "boundaries_1_3 = find_intersection(mu1, sigma1, mu3, sigma3)\n",
    "\n",
    "for boundary in boundaries_1_2:\n",
    "    if log_x_values.min() <= boundary <= log_x_values.max():\n",
    "        ax.axvline(boundary, color='black', linestyle='--')\n",
    "        ax.text(boundary, ax.get_ylim()[1] * 0.5, f'Threshold\\n{np.exp(boundary):.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "for boundary in boundaries_2_3:\n",
    "    if log_x_values.min() <= boundary <= log_x_values.max():\n",
    "        ax.axvline(boundary, color='black', linestyle='--')\n",
    "        ax.text(boundary, ax.get_ylim()[1] * 0.5, f'Threshold\\n{np.exp(boundary):.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "for boundary in boundaries_1_3:\n",
    "    if log_x_values.min() <= boundary <= log_x_values.max():\n",
    "        ax.axvline(boundary, color='black', linestyle='--')\n",
    "        ax.text(boundary, ax.get_ylim()[1] * 0.5, f'Threshold\\n{np.exp(boundary):.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "# Update legend to be in the desired order\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "order_dict = {label: handles[i] for i, label in enumerate(labels)}\n",
    "ordered_handles = [order_dict[label] for label in order]\n",
    "ax.legend(handles=ordered_handles)\n",
    "\n",
    "ax.set_title('Gaussian Fits and Histograms on Log Scale (Model Fitted on Log Scale)')\n",
    "ax.set_xlabel('Log of Artery Area')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assuming df_thickness_features is already loaded and contains the relevant data\n",
    "df_artery_type_artery_area = df_thickness_features.loc[:, [\"Image Name\", \"Artery Type\", \"Artery Area\"]]\n",
    "\n",
    "# Remove duplicates based on 'Image Name'\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area.drop_duplicates(subset='Image Name')\n",
    "\n",
    "# Ensure only the three specific artery types are included\n",
    "df_artery_type_artery_area_unique = df_artery_type_artery_area_unique[\n",
    "    df_artery_type_artery_area_unique['Artery Type'].isin(['Arterioles', 'Interlobular Arteries', 'Arcuate Arteries'])\n",
    "]\n",
    "\n",
    "# Change the order of artery types for legend and boundaries\n",
    "order = ['Arterioles', 'Interlobular Arteries', 'Arcuate Arteries']\n",
    "df_artery_type_artery_area_unique['Artery Type'] = pd.Categorical(\n",
    "    df_artery_type_artery_area_unique['Artery Type'], \n",
    "    categories=order, \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Apply log transformation to the data\n",
    "df_artery_type_artery_area_unique['Log Artery Area'] = np.log(df_artery_type_artery_area_unique['Artery Area'])\n",
    "\n",
    "# Fit Gaussian distributions\n",
    "params = {}\n",
    "for artery_type in order:\n",
    "    data = df_artery_type_artery_area_unique[df_artery_type_artery_area_unique['Artery Type'] == artery_type]['Log Artery Area']\n",
    "    mu, std = norm.fit(data)\n",
    "    params[artery_type] = (mu, std)\n",
    "\n",
    "# Function to find the intersection points of two Gaussian distributions\n",
    "def find_intersection(mu1, sigma1, mu2, sigma2):\n",
    "    a = 1/(2*sigma1**2) - 1/(2*sigma2**2)\n",
    "    b = mu2/(sigma2**2) - mu1/(sigma1**2)\n",
    "    c = mu1**2/(2*sigma1**2) - mu2**2/(2*sigma2**2) + np.log(sigma2/sigma1)\n",
    "    return np.roots([a, b, c])\n",
    "\n",
    "# Calculate intersections\n",
    "mu1, sigma1 = params['Arterioles']\n",
    "mu2, sigma2 = params['Interlobular Arteries']\n",
    "mu3, sigma3 = params['Arcuate Arteries']\n",
    "\n",
    "boundaries_1_2 = find_intersection(mu1, sigma1, mu2, sigma2)\n",
    "boundaries_2_3 = find_intersection(mu2, sigma2, mu3, sigma3)\n",
    "\n",
    "# Prepare to plot in log scale\n",
    "log_x_values = np.linspace(df_artery_type_artery_area_unique['Log Artery Area'].min(), df_artery_type_artery_area_unique['Log Artery Area'].max(), 1000)\n",
    "\n",
    "# Plotting each class's PDF and histogram on a log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for artery_type, color in zip(order, colors):\n",
    "    data = df_artery_type_artery_area_unique[df_artery_type_artery_area_unique['Artery Type'] == artery_type]['Log Artery Area']\n",
    "    mu, sigma = params[artery_type]\n",
    "    pdf = norm.pdf(log_x_values, mu, sigma)\n",
    "    \n",
    "    # Plotting histograms of the log-transformed data\n",
    "    ax.hist(data, bins=len(data)//3, density=True, alpha=0.6, color=color)\n",
    "    # Plotting the PDF\n",
    "    ax.plot(log_x_values, pdf, color=color, label=f'{artery_type}: μ={np.exp(mu):.2f}, σ={np.exp(sigma):.2f}')\n",
    "\n",
    "# Plot decision boundaries\n",
    "for boundary in boundaries_1_2:\n",
    "    if log_x_values.min() <= boundary <= log_x_values.max():\n",
    "        ax.axvline(boundary, color='black', linestyle='--')\n",
    "        ax.text(boundary, ax.get_ylim()[1] * 0.5, f'Threshold\\n{np.exp(boundary):.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "for boundary in boundaries_2_3:\n",
    "    if log_x_values.min() <= boundary <= log_x_values.max():\n",
    "        ax.axvline(boundary, color='black', linestyle='--')\n",
    "        ax.text(boundary, ax.get_ylim()[1] * 0.5, f'Threshold\\n{np.exp(boundary):.2f}', rotation=90, verticalalignment='center')\n",
    "\n",
    "# Update legend to be in the desired order\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "order_dict = {label: handles[i] for i, label in enumerate(order)}\n",
    "ordered_handles = [order_dict[label] for label in order]\n",
    "ax.legend(handles=ordered_handles)\n",
    "\n",
    "ax.set_title('Gaussian Fits and Histograms on Log Scale (Model Fitted on Log Scale)')\n",
    "ax.set_xlabel('Log of Artery Area')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_boundary_1 = np.exp(boundaries_1_2[0])\n",
    "area_boundary_2 = np.exp(boundaries_2_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_annotations(path_ann, clean=True):\n",
    "    with open(path_ann) as f:\n",
    "        exported = geojson.load(f)\n",
    "        annotations = exported['features']\n",
    "    if clean:\n",
    "        annotations = clean_geojson_annotations(annotations)\n",
    "    return annotations   \n",
    "\n",
    "    \n",
    "def get_classifications(classifications_path, sheet_name, available_sheets):\n",
    "    if sheet_name not in available_sheets:\n",
    "        logging.info(f\"Sheet {sheet_name} not found in the classifications file.\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_excel(classifications_path, sheet_name=sheet_name)\n",
    "    return df[df[\"Artery Type\"] != \"Others\"]\n",
    "\n",
    "\n",
    "def get_contours_by_classification(annotations, filter_fn, classification=\"Media\"):\n",
    "    return [np.array(ann['geometry']['coordinates'], dtype=np.int32) for ann in annotations\n",
    "            if ann[\"properties\"][\"classification\"][\"name\"] == classification and filter_fn(np.array(ann['geometry']['coordinates'], dtype=np.int32))]\n",
    "\n",
    "\n",
    "def is_contour_match_bounds(contour, bounding_box, size_ratio=1.2):\n",
    "    x, y, w, h = bounding_box\n",
    "    x_cnt, y_cnt, w_cnt, h_cnt = cv2.boundingRect(contour)\n",
    "    within_bounds = (x_cnt >= x) and (y_cnt >= y) and ((x_cnt + w_cnt) <= (x + w)) and ((y_cnt + h_cnt) <= (y + h))\n",
    "    fits_size_ratio = (w <= size_ratio * w_cnt) and (h <= size_ratio * h_cnt)\n",
    "    return within_bounds and fits_size_ratio\n",
    "\n",
    "\n",
    "def is_contour_intersecting_or_within(cnt_iner, cnt_outer):\n",
    "    return any(cv2.pointPolygonTest(cnt_outer, (int(point[0]), int(point[1])), False) >= 0 for point in cnt_iner)\n",
    "\n",
    "\n",
    "def offset_coordinates(cnt, offset):\n",
    "    return cnt - np.array([[offset]])\n",
    "\n",
    "# read thickness analysis\n",
    "def read_df_from_json(path_json):\n",
    "    df = pd.read_json(path_json, orient=\"records\", lines=True)\n",
    "    return df\n",
    "\n",
    "def save_features_to_json(features, filename):\n",
    "    df = pd.DataFrame(features)\n",
    "    df.to_json(filename, orient='records', lines=True)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "\n",
    "def get_feature_statistics(features_slide, feature_column):\n",
    "    \"\"\"Retrieve various statistics for a given feature.\"\"\"\n",
    "    feature_values = features_slide[feature_column]\n",
    "    max_value = feature_values.max()\n",
    "    avg_value = feature_values.mean()\n",
    "    p25_value = feature_values.quantile(0.25)\n",
    "    p50_value = feature_values.quantile(0.5)\n",
    "    p75_value = feature_values.quantile(0.75)\n",
    "    p90_value = feature_values.quantile(0.90)\n",
    "\n",
    "    # avg_top_25p_value = feature_values.nlargest(int(len(feature_values) * 0.25)).mean()\n",
    "    \n",
    "    return {\n",
    "        'Aggregated by Max': max_value,\n",
    "        'Aggregated by Average': avg_value,\n",
    "        'Aggregated by 25th Percentile': p25_value,\n",
    "        'Aggregated by Median': p50_value,\n",
    "        'Aggregated by 75th Percentile': p75_value,\n",
    "        'Aggregated by 90th Percentile': p90_value,\n",
    "    }\n",
    "\n",
    "def update_pat_df(pat_df, slide_filename, artery_type, feature_stats, feature_name):\n",
    "    for stat_name, value in feature_stats.items():\n",
    "        col_name = f\"{artery_type} {feature_name} {stat_name}\"\n",
    "        pat_df.loc[pat_df[\"WSI_Selected\"] == slide_filename, col_name] = value\n",
    "\n",
    "def process_artery_type(artery_type, fake_slide_basename, df_thickness_features, slide, df_classifications, pat_df, slide_basename):\n",
    "    features_slide = df_thickness_features[(df_thickness_features[\"WSI_Fake_Name\"] == fake_slide_basename) &\n",
    "                                            (df_thickness_features[\"Artery Type\"] == artery_type)]\n",
    "    # if artery_type == \"Arterioles\":\n",
    "    #     features_slide = df_thickness_features[(df_thickness_features[\"WSI_Fake_Name\"] == fake_slide_basename) &\n",
    "    #                                         (df_thickness_features[\"Artery Area\"] <= area_boundary_1)]\n",
    "    # elif artery_type == \"Arcuate Arteries\":\n",
    "    #     features_slide = df_thickness_features[(df_thickness_features[\"WSI_Fake_Name\"] == fake_slide_basename) &\n",
    "    #                                         (df_thickness_features[\"Artery Area\"] >= area_boundary_2)]\n",
    "    # elif artery_type == \"Interlobular Arteries\":\n",
    "    #     features_slide = df_thickness_features[(df_thickness_features[\"WSI_Fake_Name\"] == fake_slide_basename) &\n",
    "    #                                            (df_thickness_features[\"Artery Area\"] > area_boundary_1) &\n",
    "    #                                         (df_thickness_features[\"Artery Area\"] < area_boundary_2)]\n",
    "    # else:\n",
    "    #     print(\"Not know type\")\n",
    "    if features_slide.empty:\n",
    "        logging.warning(f\"No feature data for {artery_type} in {fake_slide_basename}.\")\n",
    "        return\n",
    "\n",
    "    feature_columns = [\"Media Average\", \"Media Peak Height\", \"Media Area\", \"Intima Average\", \"Intima Peak Height\", \"Intima Area\", \"Ratio Average\", \"Ratio Peak Height\", \"Ratio Intima/Media Area\"]\n",
    "    feature_columns = [\"Media Average\", \"Media Peak Height\", \"Media Area\", \"Intima Average\", \"Intima Peak Height\", \"Intima Area\", \"Ratio Average\", \"Ratio Peak Height\", \"Ratio Intima/Media Area\"]\n",
    "    for feature_column in feature_columns:\n",
    "        feature_stats = get_feature_statistics(features_slide, feature_column)\n",
    "        update_pat_df(pat_df, slide_filename, artery_type, feature_stats, feature_column)\n",
    "\n",
    "    \n",
    "def process_single_slide(slide_filename, df_thickness_features, available_sheetnames, pat_df):\n",
    "    slide_basename = os.path.splitext(slide_filename)[0]\n",
    "    fake_slide_basename = pat_df.loc[pat_df[\"WSI_Selected\"] == slide_filename, \"WSI_Selected_Fake_Name\"].iloc[0]\n",
    "\n",
    "    df_classifications = get_classifications(VESSEL_CLASSIFICATION_PATH, slide_basename, available_sheetnames)\n",
    "    if df_classifications.empty:\n",
    "        logging.warning(f\"No classifications found for {slide_basename}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    slide_path = os.path.join(TRI_DIR, slide_filename)\n",
    "    # slide = openslide.OpenSlide(slide_path)\n",
    "    slide = None\n",
    "\n",
    "    for artery_type in [\"Interlobular Arteries\", \"Arcuate Arteries\", \"Arterioles\",]:\n",
    "        process_artery_type(artery_type, fake_slide_basename, df_thickness_features, slide, df_classifications, pat_df, slide_basename)\n",
    "\n",
    "\n",
    "available_sheetnames = pd.ExcelFile(VESSEL_CLASSIFICATION_PATH).sheet_names\n",
    "\n",
    "for i, slide_filename in enumerate(pat_df[\"WSI_Selected\"]):\n",
    "    logging.info(f\"Processing: {i+1}/{len(pat_df['WSI_Selected'])} {slide_filename}\")\n",
    "    process_single_slide(slide_filename, df_thickness_features, available_sheetnames, pat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df.to_csv(\"pat_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = pd.read_csv(\"pat_df.csv\")\n",
    "pat_df = pat_df[(pat_df[\"WSI_Selected\"].notna()) \n",
    "                & (pat_df[\"ESRDorEGFR40BX_LR\"].notna()) \n",
    "                & (pat_df[\"DaysBXtoESRDorEGFR40_LR\"].notna())]\n",
    "\n",
    "pat_df\n",
    "pat_df['DaysBXtoESRDorEGFR40_LR'] = pd.to_numeric(pat_df['DaysBXtoESRDorEGFR40_LR'], errors='coerce')\n",
    "pat_df['ESRDorEGFR40BX_LR'] = pat_df['ESRDorEGFR40BX_LR'].map({'1: Yes': 1, '0: No': 0}).astype(int)\n",
    "# pat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def survival_analysis(pat_df_selected, feature_name, percentile, ax, c=\"\"):\n",
    "    # Determine the specified percentile value of the feature\n",
    "    threshold = pat_df_selected[feature_name].quantile(percentile)\n",
    "\n",
    "    # Create a new column for group based on whether the feature value is above the specified percentile\n",
    "    if percentile == .5:\n",
    "        pat_df_selected['Group'] = np.where(pat_df_selected[feature_name] > threshold, f'Above Median', f'Below or Equal Median')\n",
    "    else:\n",
    "        pat_df_selected['Group'] = np.where(pat_df_selected[feature_name] > threshold, f'Above {percentile*100}th', f'Below or Equal {percentile*100}th')\n",
    "\n",
    "    # Prepare data for log-rank test\n",
    "    durations = []\n",
    "    events = []\n",
    "    labels = []\n",
    "    assert(len(pat_df_selected['Group'].unique()) == 2)\n",
    "    for group in sorted(pat_df_selected['Group'].unique()):\n",
    "        df_sub = pat_df_selected[pat_df_selected['Group'] == group]\n",
    "        T = df_sub[\"DaysBXtoESRDorEGFR40_LR\"]\n",
    "        E = df_sub[\"ESRDorEGFR40BX_LR\"]\n",
    "        durations.append(T)\n",
    "        events.append(E)\n",
    "        labels.append(group)\n",
    "\n",
    "        if len(T) > 0:  # Ensure there is data to fit\n",
    "            km = KaplanMeierFitter()\n",
    "            km.fit(durations=T, event_observed=E, label=group)\n",
    "            km.plot_survival_function(ax=ax, show_censors=True)\n",
    "\n",
    "    # Perform and display log-rank test if there are at least two groups\n",
    "    # if len(labels) > 1:\n",
    "    #     from itertools import combinations\n",
    "    #     p_values = []\n",
    "    #     for (i, j) in combinations(range(len(labels)), 2):\n",
    "    #         result = logrank_test(durations[i], durations[j], event_observed_A=events[i], event_observed_B=events[j])\n",
    "    #         p_values.append((f\"{labels[i]} vs {labels[j]}\", result.p_value))\n",
    "\n",
    "        # ax.set_title(f\"{feature_name}\\n\" + \"\\n\".join([f\"{pair[0]}: p={pair[1]:.4f}\" for pair in p_values]), fontsize=12)\n",
    "        # print(p_values)\n",
    "    result = logrank_test(durations[0], durations[1], event_observed_A=events[0], event_observed_B=events[1])\n",
    "    p_str = \"p<0.05\" if result.p_value < 0.05 else f\"p={result.p_value:.3f}\"\n",
    "\n",
    "    ax.set_xlabel('Days from Biopsy to ESRD or EGFR < 40', fontsize=12)\n",
    "    ax.set_ylabel('Survival Probability', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"({c}){feature_name}: \\n log rank test: {p_str}\", fontsize=12, y=-0.25, pad=-5)\n",
    "    # ax.set_title(f\"({c}){feature_name}, p={result.p_value:.4f}\", fontsize=12, y=-0.25, pad=-5)\n",
    "\n",
    "# Main processing loop\n",
    "def survival_analysis_artery_type(artery_type, percentiles=[]):\n",
    "    features = [col for col in pat_df.columns if artery_type in col]\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        # if \"Media \" in feature: continue\n",
    "        # if \"25\" in feature: continue\n",
    "        # if not \"Average\" in feature: continue\n",
    "        # if not \"Ratio Average\" in feature: continue\n",
    "\n",
    "        # if not ((\"max\" in feature) or (\"90th_percentile\" in feature)): continue\n",
    "        # if not ((\"Intima Average max\" in feature) or (\"Intima Peak Height max\" in feature)): continue\n",
    "        # print(feature)\n",
    "        fig, axs = plt.subplots(1, len(percentiles), figsize=(20, 5))\n",
    "        pat_df_selected = pat_df[pat_df[feature].notna()]  # Filter non-null values for the feature\n",
    "        if not pat_df_selected.empty:\n",
    "            for jdx, percentile in enumerate(percentiles):\n",
    "                survival_analysis(pat_df_selected, feature, percentile, axs[jdx])\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "# from lifelines import CoxPHFitter\n",
    "# def cox_regression_analysis(pat_df, features):\n",
    "    \n",
    "#     pat_df_selected = pat_df.dropna(subset=features + [\"DaysBXtoESRDorEGFR40_LR\", \"ESRDorEGFR40BX_LR\"])\n",
    "#     cph = CoxPHFitter()\n",
    "#     cph.fit(pat_df_selected[features + [\"DaysBXtoESRDorEGFR40_LR\", \"ESRDorEGFR40BX_LR\"]], \n",
    "#             duration_col=\"DaysBXtoESRDorEGFR40_LR\", \n",
    "#             event_col=\"ESRDorEGFR40BX_LR\")\n",
    "#     cph.print_summary()\n",
    "#     cph.plot()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# for artery_type in [\"Arterioles\", \"Interlobular Arteries\", \"Arcuate Arteries\"]:\n",
    "#     survival_analysis_artery_type(artery_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.utils_constants import ARTERY_TYPES, DISEASE_TYPES, VESSEL_NEPTUNE_PAT_INFO_W_SCORE_W_FEATURE_PATH\n",
    "\n",
    "# VESSEL_NEPTUNE_PAT_INFO_W_SCORE_W_FEATURE_PATH = '/DataMount/NEPTUNE/Clinical_Data/Barisoni_NEPTUNE_clinical_20230620_Vessel_20240710_W_SCORE_W_FEATURE_measurements_lumen_convex.csv'\n",
    "VESSEL_NEPTUNE_PAT_INFO_W_SCORE_W_FEATURE_PATH = '/DataMount/NEPTUNE/Clinical_Data/Barisoni_NEPTUNE_clinical_20230620_Vessel_20240710_W_SCORE_W_FEATURE_measurements.csv'\n",
    "\n",
    "def binary_map(x, positive_value):\n",
    "    return 1 if x == positive_value else 0\n",
    "\n",
    "pat_df = pd.read_csv(VESSEL_NEPTUNE_PAT_INFO_W_SCORE_W_FEATURE_PATH)\n",
    "CLININCAL_FEATURES = ['PAT_Sex', 'PAT_Hispanic', 'PAT_Cohort', 'PAT_Race', 'PAT_AgeV3', 'eGFRatBx', 'UPCRatBx', \n",
    "                      'Immunosupression_30dBfOrAtBx', 'ArterioSclerosis', 'ArterialHyalinosis']\n",
    "\n",
    "pat_df.dropna(subset = CLININCAL_FEATURES, inplace = True)\n",
    "pat_df['DaysBXtoESRDorEGFR40_LR'] = pd.to_numeric(pat_df['DaysBXtoESRDorEGFR40_LR'], errors='coerce')\n",
    "pat_df['ESRDorEGFR40BX_LR'] = pat_df['ESRDorEGFR40BX_LR'].map({'1: Yes': 1, '0: No': 0}).astype(int)\n",
    "\n",
    "# # Applying the function to various columns\n",
    "# pat_df['PAT_Hispanic'] = pat_df['PAT_Hispanic'].apply(lambda x: binary_map(x, '1: Hispanic or Latino'))\n",
    "# pat_df['Immunosupression_30dBfOrAtBx'] = pat_df['Immunosupression_30dBfOrAtBx'].apply(lambda x: binary_map(x, '1: Yes'))\n",
    "# pat_df['PAT_Race'] = pat_df['PAT_Race'].apply(lambda x: binary_map(x, '3: Black/African American'))\n",
    "# pat_df['PAT_Sex'] = pat_df['PAT_Sex'].apply(lambda x: binary_map(x, '2: Female'))\n",
    "# pat_df['PAT_Cohort'] = pat_df['PAT_Cohort'].apply(lambda x: binary_map(x, '2 - MCD'))\n",
    "# pat_df['ArterioSclerosis'] = pat_df['ArterioSclerosis'].map(SEVERITY_MAPPING)\n",
    "# pat_df['ArterialHyalinosis'] = pat_df['ArterialHyalinosis'].map(SEVERITY_MAPPING)\n",
    "\n",
    "# covariates_to_normalize = ['PAT_AgeV3', 'eGFRatBx', 'UPCRatBx', 'ArterioSclerosis', 'ArterialHyalinosis']  # add numerical columns here\n",
    "# scaler = StandardScaler() # choose a scaler?\n",
    "# pat_df[covariates_to_normalize] = scaler.fit_transform(pat_df[covariates_to_normalize]) # in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTERY_TYPES = [\n",
    "    # 'Arterioles', \n",
    "                # 'Interlobular_Arteries',\n",
    "                  'Arcuate_Arteries'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    # 'Intima_Average',  'Intima_Peak_Height', \n",
    "    # 'Media_Average', 'Media_Peak_Height',\n",
    "    # 'Ratio_Average', 'Ratio_Peak_Height',\n",
    "        'Ratio_Average'\n",
    "    ]\n",
    "\n",
    "agg_feature_names = []\n",
    "# for agg_metric in [\"Max\", \"Median\", \"75th\"]:\n",
    "for agg_metric in [\"Median\", \"75th\"]:\n",
    "    for f in feature_names:\n",
    "        agg_feature_names.append('_'.join([agg_metric, f]))\n",
    "\n",
    "# fig, axs = plt.subplots(3, len(agg_feature_names), figsize=(7*len(agg_feature_names), 10))\n",
    "alphabets=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "count = 0\n",
    "for idx, artery_type in enumerate(ARTERY_TYPES):\n",
    "    for idy, feature in enumerate(agg_feature_names):\n",
    "        feature_name = feature + \"_in_\" + artery_type\n",
    "        pat_df_selected = pat_df[pat_df[feature_name].notna()]  # Filter non-null values for the feature\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(8, 5))\n",
    "        survival_analysis(pat_df_selected, feature_name, 0.5, axs, alphabets[count])\n",
    "        plt.show()\n",
    "        count += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\" Ratio Average Aggregated by Median\", \n",
    "            \" Ratio Average Aggregated by 75th Percentile\", \n",
    "        #     \" Ratio Average Aggregated by Max\", \n",
    "            ]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(features)*3, figsize=(5.5*len(features*3), 6))\n",
    "alphabets=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "count = 0\n",
    "for idx, artery_type in enumerate(ARTERY_TYPES):\n",
    "    for idy, feature in enumerate(features):\n",
    "        feature_name = artery_type + feature\n",
    "        pat_df_selected = pat_df[pat_df[feature_name].notna()]  # Filter non-null values for the feature\n",
    "        survival_analysis(pat_df_selected, feature_name, 0.5, axs[count], alphabets[count])\n",
    "        count += 1\n",
    "fig.text(0.5, 0, \"Figure 2. Kaplan-Meier curves comparing biopsies based on the feature of intima-media thickness ratio average in different vessel types, aggregated by different metrics. The comparison is between biopsies with values above and below or equal to the median feature values across biopsies.\",\n",
    "        ha='center', va='top', fontsize=15, wrap=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"Arterioles Ratio Average Aggregated by Median\", \n",
    "            \"Arterioles Ratio Average Aggregated by 75th Percentile\", \n",
    "            \"Arterioles Ratio Average Aggregated by Max\", \n",
    "            ]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(features), figsize=(15*len(features), 5), dpi=50)\n",
    "alphabets=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "count = 0\n",
    "for idy, feature in enumerate(features):\n",
    "    feature_name = feature\n",
    "    pat_df_selected = pat_df[pat_df[feature_name].notna()]  # Filter non-null values for the feature\n",
    "    survival_analysis(pat_df_selected, feature_name, 0.5, axs[idy], alphabets[count])\n",
    "    count += 1\n",
    "fig.text(0.5, 0, \"Figure 2. Kaplan-Meier curves comparing biopsies based on the feature of intima-media thickness ratio average in arterioles, aggregated by different metrics. The comparison is between biopsies with values above and below or equal to the median feature values across biopsies.\",\n",
    "        ha='center', va='top', fontsize=13, wrap=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival_analysis_artery_type(\"Arterioles\", percentiles = [0.5, 0.75, 0.85, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival_analysis_artery_type(\"Interlobular Arteries\", percentiles = [0.25, 0.75, 0.85, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival_analysis_artery_type(\"Arcuate Arteries\", [0.25, 0.75, 0.85, 0.95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
